= How to  deploy a Kubernetes cluster on RHEL 7.5 flavour of LinuxONE instances?

For this documentation two LinuxONE instances has been used for deployment and testing pupose. One can extend it to any number of worker nodes.
Out of these two instances one acts as master and other acts as a worker. Create two instances on LinuxONE community cloud and ssh using the respective keys.


A kubernetes cluster consists of these essential components

....
1. Etcd (distributed key-value storage)
2. Server
3. Scheduler
4. Controller manager
5. Kubectl (command line client for kubernetes)
5. Docker (container)
6. Kubelet
7. Kube-proxy
8. Flannel
....

The following components have been used.
[%header,cols=2*,width="30%"]
|===
|Component
|Version
|Instances
|RHEL 7.5 flavour of L1CC
|ETCD
|3.2.22
|Docker CE
|17.05
|Kubernetes
|1.9.8
|Flannel
|0.10.0
|===

== Prerequisites


NOTE: All the commands have been run as root user. There may permission issues at some points. So it is better if we login as root user from the start itself.

Also copy the worker node keys to the master node because you will need them to transfer certificates and config files.
....
scp -i master.pem <PATH_OF_WORKER_NODES>/{worker1.pem,worker2.pem} root@148.100.98.174:/root
....

After you ssh into your instance check the rhel version using the below command and wait for some time. Now  rerun the same command, the rhel version gets updated.

....
cat /etc/os-release
....

To be able to download packages you need to register your instance and for that you need a RHEL account. Also enable the repos from which you can download
the kubernetes components.

....
subscription-manager register
subscription-manager attach
subscription-manager repos --enable=rhel-7-for-system-z-extras-rpms
subscription-manager repos --list-enabled
....


== Binaries

Set the following variables on both the nodes
....
TAG=v1.9.8
URL=https://storage.googleapis.com/kubernetes-release/release/$TAG/bin/linux/s390x
....

*Binaries to be installed on Master Node*

....
curl -# -L -o /usr/bin/kube-apiserver $URL/kube-apiserver
curl -# -L -o /usr/bin/kube-controller-manager $URL/kube-controller-manager
curl -# -L -o /usr/bin/kube-scheduler $URL/kube-scheduler
curl -# -L -o /usr/bin/kubectl $URL/kubectl
....

Make them executable

....
chmod +x /usr/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl}
....

*Binaries to be installed on Worker Node*

....
curl -# -L -o /usr/bin/kube-proxy $URL/kube-proxy
curl -# -L -o /usr/bin/kubelet $URL/kubelet
....

Make them executable

....
chmod +x /usr/bin/{kubelet,kube-proxy}
....

*Swap space*

The swap space on all kubernetes master and nodes should be disabled. If swap is
not disabled, kubelet service will not start on the master and nodes.
This is done to tightly pack instances. All deployments should be pinned with
CPU/memory limits. So if the scheduler sends a pod to a machine it should never
use swap space at all. Swap space slows down things. Its mainly done
for improving  performance.

....
swapoff -a
....

Also,  comment the entries related to swap space in  /etc/fstab file so that
after a reboot also the swap space remains disabled. You can open the file and
comment the swap related lines or use the below command.

....
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
....

By default selinux and firewalld are disabled. You can check them using this

*Selinux*

....
cat /etc/selinux/config
....

*Firewalld*
....
systemctl status firewalld
....

== Folders

*Make folders on master node*
....
mkdir -p /srv/kubernetes
mkdir -p /var/lib/flanneld
mkdir -p /var/lib/{kube-scheduler,kube-controller-manager}
....

*Make folders on worker node or nodes*
....
mkdir -p /srv/kubernetes
mkdir -p /var/lib/flanneld
mkdir -p /var/lib/{kubelet}
....



NOTE: In this documnetation whereever 148.100.98.173 has been used you should use your master ip there and inplace of 148.100.98.174 use your worker ip.

== Certificates

....
cd /srv/kubernetes
openssl genrsa -out ca-key.pem 2048
openssl req -x509 -new -nodes -key ca-key.pem -days 7200 -out ca.pem -subj "/CN=kube-ca"
....

*Openssl Config for generating certificates for master node*

....
cat > openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 127.0.0.1
IP.2 = 148.100.98.173
IP.3 = 100.65.0.1 #Service IP
EOF
....

*Kube-apiserver certificates*

....
openssl genrsa -out apiserver-key.pem 2048
openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 7200 -extensions v3_req -extfile openssl.cnf
....

*Kube-controller certificates*

....
openssl genrsa -out kube-controller-manager-key.pem 2048
openssl req -new -key kube-controller-manager-key.pem -out kube-controller-manager.csr -subj "/CN=kube-controller-manager"
openssl x509 -req -in kube-controller-manager.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-controller-manager.pem -days 7200
....

*Kube-scheduler certificates*

....
openssl genrsa -out kube-scheduler-key.pem 2048
openssl req -new -key kube-scheduler-key.pem -out kube-scheduler.csr -subj "/CN=kube-scheduler"
openssl x509 -req -in kube-scheduler.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-scheduler.pem -days 7200
....

*Admin certificates*

....
openssl genrsa -out admin-key.pem 2048
openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=admin"
openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 7200
....

*Openssl Config for generating certificates for worker node*

....
cat > worker_openssl.cnf << EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = 148.100.98.174
EOF
....

NOTE: If you have more than one worker node then add their ip addressess in the below configuration file as IP.2= , IP.3= ...


*Kubelet certificates*
....
openssl genrsa -out kubelet-key.pem 2048
openssl req -new -key kubelet-key.pem -out kubelet.csr -subj "/CN=kubelet" -config worker_openssl.cnf
openssl x509 -req -in kubelet.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kubelet.pem -days 7200 -extensions v3_req -extfile worker_openssl.cnf
....

*Kube-proxy certificates*
....
openssl genrsa -out kube-proxy-key.pem 2048
openssl req -new -key kube-proxy-key.pem -out kube-proxy.csr -subj "/CN=kube-proxy"
openssl x509 -req -in kube-proxy.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-proxy.pem -days 7200
....

Before you generate the worker certificates do note that kubeworker.novalocal is the hostname of my worker machine. So do change it according to your worker node's hostname.

*Worker certificates*
....
openssl genrsa -out kubeworker.novalocal-worker-key.pem 2048
WORKER_IP=148.100.98.174 openssl req -new -key kubeworker.novalocal-worker-key.pem -out kubeworker.novalocal-worker.csr -subj "/CN=system:node:kubeworker.novalocal" -config worker_openssl.cnf
WORKER_IP=148.100.98.174 openssl x509 -req -in kubeworker.novalocal-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kubeworker.novalocal-worker.pem -days 7200 -extensions v3_req -extfile worker_openssl.cnf
....

*Config file for generating etcd peer certificates*

....
cat > etcd_openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = clientAuth,serverAuth
subjectAltName = @alt_names
[alt_names]
IP.1 = 148.100.98.173
EOF
....

*Etcd certificates*

....
openssl genrsa -out etcd.key 2048
openssl req -new -key etcd.key -out etcd.csr -subj "/CN=etcd" -extensions v3_req -config etcd_openssl.cnf -sha256
openssl x509 -req -sha256 -CA ca.pem -CAkey ca-key.pem -CAcreateserial -in etcd.csr -out etcd.crt -extensions v3_req -extfile etcd_openssl.cnf -days 7200
....

Copy the required keys to the worker node/nodes

....
scp -i <path_where_your_worker_keys_are> /srv/kubernetes/{ca.pem,etcd.crt,etcd.key,kubelet.pem,kubelet-key.pem} root@148.100.98.174:/srv/kubernetes/
....

== Configuation files
*Kubeconfig files for various components*

*admin kubeconfig*

....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.98.173:6443
kubectl config set-credentials admin --client-certificate=/srv/kubernetes/admin.pem --client-key=/srv/kubernetes/admin-key.pem --embed-certs=true --token=$TOKEN
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=admin
kubectl config use-context linux1.k8s
cat ~/.kube/config
....

*kube-controller-manager kubeconfig*

....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.98.173:6443 --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-credentials kube-controller-manager --client-certificate=/srv/kubernetes/kube-controller-manager.pem --client-key=/srv/kubernetes/kube-controller-manager-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-controller-manager --kubeconfig=/var/lib/kube-controller-manager/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
....

*kube-scheduler kubeconfig*

....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.98.173:6443 --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-credentials kube-scheduler --client-certificate=/srv/kubernetes/kube-scheduler.pem --client-key=/srv/kubernetes/kube-scheduler-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-scheduler --kubeconfig=/var/lib/kube-scheduler/kubeconfig; kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-scheduler/kubeconfig
....

*kubelet kubeconfig*

....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.98.173:6443 --kubeconfig=/srv/kubernetes/kubeworker.novalocal-worker.kubeconfig
kubectl config set-credentials kubeworker.novalocal --client-certificate=/srv/kubernetes/kubeworker.novalocal-worker.pem --client-key=/srv/kubernetes/kubeworker.novalocal-worker-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/srv/kubernetes/kubeworker.novalocal-worker.kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kubeworker.novalocal --kubeconfig=/srv/kubernetes/kubeworker.novalocal-worker.kubeconfig
kubectl config use-context linux1.k8s --kubeconfig=/srv/kubernetes/kubeworker.novalocal-worker.kubeconfig
....

Copy the required config files to the worker node/nodes

....
scp -i <path_where_your_worker_keys_are> kubeworker.novalocal-worker.kubeconfig root@148.100.98.174:/var/lib/kubelet/kubelet.kubeconfig
....

== Etcd

*About etcd*

It is a distributed storage device used to store the state of the cluster. All other components are stateless. A state is stored in the
form of key-value pair.

Package Installation

....
yum install etcd
....

Modify the service file in the /usr/lib/systemd/system/etcd.service so that it should look like this after the modifications

....
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
Environment="ETCD_UNSUPPORTED_ARCH=S390X"
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
User=etcd
# set GOMAXPROCS to number of processors

ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\"${ETCD_NAME}\"  \
--data-dir=\"${ETCD_DATA_DIR}\" \
--listen-client-urls=\"${ETCD_LISTEN_CLIENT_URLS}\" \
--cert-file=\"${ETCD_CERT_FILE}\" \
--key-file=\"${ETCD_KEY_FILE}\" \
--peer-cert-file=\"${ETCD_PEER_CERT_FILE}\" \
--peer-key-file=\"${ETCD_PEER_KEY_FILE}\" \
--trusted-ca-file=\"${ETCD_TRUSTED_CA_FILE}\"  \
--peer-trusted-ca-file=\"${ETCD_TRUSTED_CA_FILE}\"  \
--peer-client-cert-auth \
--client-cert-auth \
--initial-advertise-peer-urls=\"${ETCD_INITIAL_ADVERTISE_PEER_URLS}\"  \
--listen-peer-urls=\"${ETCD_LISTEN_PEER_URLS}\" \
--advertise-client-urls=\"${ETCD_ADVERTISE_CLIENT_URLS}\"  \
--initial-cluster-token=\"${ETCD_INITIAL_CLUSTER_TOKEN}\" \
--initial-cluster=\"${ETCD_INITIAL_CLUSTER}\" \
--initial-cluster-state=\"${ETCD_INITIAL_CLUSTER_STATE}\""

Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
....

Modify the configuration file in the */etc/etcd/etcd.conf* and it should look like this after all the modifications. But do note to change the ip address.

....
#[Member]
#ETCD_CORS=""
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
#ETCD_WAL_DIR=""
ETCD_LISTEN_PEER_URLS="https://148.100.98.173:2380"
ETCD_LISTEN_CLIENT_URLS="https://148.100.98.173:2379"
#ETCD_MAX_SNAPSHOTS="5"
#ETCD_MAX_WALS="5"
ETCD_NAME="default"
#ETCD_SNAPSHOT_COUNT="100000"
#ETCD_HEARTBEAT_INTERVAL="100"
#ETCD_ELECTION_TIMEOUT="1000"
#ETCD_QUOTA_BACKEND_BYTES="0"
#ETCD_MAX_REQUEST_BYTES="1572864"
#ETCD_GRPC_KEEPALIVE_MIN_TIME="5s"
#ETCD_GRPC_KEEPALIVE_INTERVAL="2h0m0s"
#ETCD_GRPC_KEEPALIVE_TIMEOUT="20s"
#
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://148.100.98.173:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://148.100.98.173:2379"
#ETCD_DISCOVERY=""
#ETCD_DISCOVERY_FALLBACK="proxy"
#ETCD_DISCOVERY_PROXY=""
#ETCD_DISCOVERY_SRV=""
ETCD_INITIAL_CLUSTER="default=https://148.100.98.173:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-0"
ETCD_INITIAL_CLUSTER_STATE="new"
#ETCD_STRICT_RECONFIG_CHECK="true"
#ETCD_ENABLE_V2="true"
#
#[Proxy]
#ETCD_PROXY="off"
#ETCD_PROXY_FAILURE_WAIT="5000"
#ETCD_PROXY_REFRESH_INTERVAL="30000"
#ETCD_PROXY_DIAL_TIMEOUT="1000"
#ETCD_PROXY_WRITE_TIMEOUT="5000"
#ETCD_PROXY_READ_TIMEOUT="0"
#
#[Security]
ETCD_CERT_FILE="/srv/kubernetes/etcd.crt"
ETCD_KEY_FILE="/srv/kubernetes/etcd.key"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_TRUSTED_CA_FILE="/srv/kubernetes/ca.pem"
ETCD_AUTO_TLS="false"
ETCD_PEER_CERT_FILE="/srv/kubernetes/etcd.crt"
ETCD_PEER_KEY_FILE="/srv/kubernetes/etcd.key"
ETCD_PEER_CLIENT_CERT_AUTH="true"
#ETCD_PEER_TRUSTED_CA_FILE=""
ETCD_PEER_AUTO_TLS="false"
#
#[Logging]
ETCD_DEBUG="true"
ETCD_LOG_PACKAGE_LEVELS="DEBUG"
#ETCD_LOG_OUTPUT="default"
#
#[Unsafe]
#ETCD_FORCE_NEW_CLUSTER="false"
#
#[Version]
#ETCD_VERSION="false"
#ETCD_AUTO_COMPACTION_RETENTION="0"
#
#[Profiling]
#ETCD_ENABLE_PPROF="false"
#ETCD_METRICS="basic"
#
#[Auth]
#ETCD_AUTH_TOKEN="simple"
....


NOTE: Before you start your etcd instance make sure you flush your iptables. You have to do this every time you need to start your etcd.

....
iptables --flush
....

*Commands to start, check status of etcd*

....
systemctl enable etcd
systemctl start etcd
systemctl status etcd
....

*Testing etcd*
....
etcdctl --endpoints https://148.100.98.173:2379 \
  --ca-file=/srv/kubernetes/ca.pem \
  --cert-file=/srv/kubernetes/etcd.crt \
  --key-file=/srv/kubernetes/etcd.key \
  cluster-health
....

Output
....
member ddd902eabd9d33b7 is healthy: got healthy result from http://148.100.98.173:2379
cluster is healthy
....

....
etcdctl --endpoints https://148.100.98.173:2379 \
  --ca-file=/srv/kubernetes/ca.pem \
  --cert-file=/srv/kubernetes/etcd.crt \
  --key-file=/srv/kubernetes/etcd.key \
  member list
....

....
ddd902eabd9d33b7: name=default peerURLs=http://148.100.98.173:2380 clientURLs=http://148.100.98.173:2379 isLeader=true
....


== Docker

You need not install *Docker CE* because it is present in /usr/local/bin folder. But this docker does have a service file related to it.
So remove the existing service file and create a new one.

....
rm -rf /usr/lib/systemd/system/docker*
....

Make an empty file in */etc/sysconfig/* with name *docker* so later you can add the docker configuration options.

Now create both of the following files in the /etc/systemd/system/ folder.

*docker.service*
....
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.com
After=network.target docker.socket
Requires=docker.socket

[Service]
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
EnvironmentFile=/etc/sysconfig/docker
PIDFile=/var/run/docker.pid
ExecStart=/usr/local/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 -G docker
#ExecStart=/usr/local/bin/dockerd -H --storage-driver=devicemapper fd:// $DOCKER_OPTS
MountFlags=slave
LimitNOFILE=1048576
LimitNPROC=1048576
LimitCORE=infinity
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

[Install]
WantedBy=multi-user.target
....

*docker.socket*
....
[Unit]
Description=Docker Socket for the API
PartOf=docker.service

[Socket]
ListenStream=/var/run/docker.sock
SocketMode=0660
# A Socket(User|Group) replacement workaround for systemd <= 214
ExecStartPost=/usr/bin/chown root:docker /var/run/docker.sock
....

Reload configuration, remove the previous symbolic links, form new ones

....
systemctl daemon-reload && systemctl disable docker.service && systemctl enable docker.service
....

Run these two below commands so that a non-root user will be able to use docker.
....
groupadd docker
....

....
usermod -aG docker linux1
....

*Command for getting docker started* +

....
systemctl start docker
....

== Flannel

Flannel is networking overlay layer designed for kubernetes but it is also used as a general purpose SDN.
Flannel does this by creating a flat network over the entire cluster which runs above the host network overlay network. So it has to be installed on all the nodes.
So by this overlay network each container gets an IP and this makes the container to container communication easy. If two containers are on the same machine then
they can use the docker bridge otherwise they use the encaptulation and UDP in order to communicate with each other.

....
curl -# -L -o /usr/bin/flanneld https://github.com/coreos/flannel/releases/download/v0.10.0/flanneld-s390x
chmod +x /usr/bin/flanneld
....

Flannel does use etcd for mapping subnet to host. So by running the below command etcd will know that flannel will use it's service.
Do this on the master node only as etcd is running on the master.
....
etcdctl --endpoints https://148.100.98.173:2379 --cert-file /srv/kubernetes/etcd.crt --key-file /srv/kubernetes/etcd.key --ca-file /srv/kubernetes/ca.pem set /coreos.com/network/config '{ "Network": "100.64.0.0/16", "SubnetLen": 24, "Backend": {"Type": "vxlan"} }'
....

Create a flannel service file with your etcd endpoints.

....
cat > /etc/systemd/system/flanneld.service << EOF
[Unit]
Description=Network fabric for containers
Documentation=https://github.com/coreos/flannel
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
Restart=always
RestartSec=5
ExecStart= /usr/bin/flanneld \\
  -etcd-endpoints=https://148.100.98.173:2379 \\
  -ip-masq=true \\
  -subnet-file=/var/lib/flanneld/subnet.env \\
  -etcd-cafile=/srv/kubernetes/ca.pem \\
  -etcd-certfile=/srv/kubernetes/etcd.crt \\
  -etcd-keyfile=/srv/kubernetes/etcd.key

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start, check status flanneld*

....
systemctl enable flanneld
systemctl start flanneld
systemctl status flanneld
....

NOTE: If flannel bridge has been established then you should be able to ping one flannel node from another but before you do that flush your iptables.

Do add the below line  by substituting the values of bip and mtu to the docker execstart of docker service. You can get the values of bip and mtu in the env file of flanneld located
in /var/lib/flanneld. The value of bip is that of FLANNEL_SUBNET and the value of mtu is that of FLANNEL_MTU.

....
--bip= --mtu= --iptables=false --ip-masq=false --ip-forward=true"
....

....
ip link del docker0
....

After adding the above line reload the docker daemon and restart it.

*Kube-apiserver*

*Setting up kube-apiserver as systemd service*

....
cat > /etc/systemd/system/kube-apiserver.service << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target etcd.service flanneld.service

[Service]
EnvironmentFile=-/var/lib/flanneld/subnet.env
ExecStart=/usr/bin/kube-apiserver \\
 --bind-address=0.0.0.0 \\
 --advertise-address=148.100.98.173\\
 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota \\
 --anonymous-auth=false \\
 --apiserver-count=1 \\
 --authorization-mode=Node,RBAC,AlwaysAllow \\
 --authorization-rbac-super-user=admin \\
 --etcd-cafile=/srv/kubernetes/ca.pem \\
 --etcd-certfile=/srv/kubernetes/etcd.crt \\
 --etcd-keyfile=/srv/kubernetes/etcd.key \\
 --etcd-servers=https://148.100.98.173:2379 \\
 --enable-swagger-ui=true \\
 --insecure-bind-address=0.0.0.0 \\
 --kubelet-certificate-authority=/srv/kubernetes/ca.pem \\
 --kubelet-client-certificate=/srv/kubernetes/kubelet.pem \\
 --kubelet-client-key=/srv/kubernetes/kubelet-key.pem \\
 --kubelet-https=true \\
 --client-ca-file=/srv/kubernetes/ca.pem \\
 --runtime-config=api/all=true,batch/v2alpha1=true,rbac.authorization.k8s.io/v1alpha1=true \\
 --secure-port=6443 \\
 --service-cluster-ip-range=100.65.0.0/24 \\
 --storage-backend=etcd3 \\
 --tls-cert-file=/srv/kubernetes/apiserver.pem \\
 --tls-private-key-file=/srv/kubernetes/apiserver-key.pem \\
 --tls-ca-file=/srv/kubernetes/ca.pem \\
 --logtostderr=true
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kube-apiserver*
....
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
....

Run the following command to check the kubectl version and it also ensures that the server has been connected it you see the server version.

....
kubectl version
....

Output

....
Client Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.8", GitCommit:"c138b85178156011dc934c2c9f4837476876fb07", GitTreeState:"clean", BuildDate:"2018-05-21T19:01:12Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/s390x"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.8", GitCommit:"c138b85178156011dc934c2c9f4837476876fb07", GitTreeState:"clean", BuildDate:"2018-05-21T18:53:18Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/s390x"}
....


*Setting up Kube-scheduler*

....
cat > /etc/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/bin/kube-scheduler \\
  --leader-elect=true \\
  --kubeconfig=/var/lib/kube-scheduler/kubeconfig \\
  --master=https://148.100.98.173:6443
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

....

*Commands to start and check status of kube-scheduler*

....
sudo systemctl enable kube-scheduler
sudo systemctl start kube-scheduler
sudo systemctl status kube-scheduler
....


*Setting up Kube-controller manager*

....
cat > /etc/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/bin/kube-controller-manager \\
      --address=0.0.0.0 \\
      --allocate-node-cidrs=true \\
      --attach-detach-reconcile-sync-period=1m0s \\
      --cluster-cidr=100.64.0.0/16 \\
      --cluster-name=k8s.virtual.local \\
      --cluster-signing-cert-file=/srv/kubernetes/ca.pem \\
      --cluster-signing-key-file=/srv/kubernetes/ca-key.pem \\
      --configure-cloud-routes=false \\
      --kubeconfig=/var/lib/kube-controller-manager/kubeconfig \\
      --leader-elect=true \\
      --master=https://148.100.98.173:6443 \\
      --root-ca-file=/srv/kubernetes/ca.pem \\
      --service-account-private-key-file=/srv/kubernetes/apiserver-key.pem \\
      --service-cluster-ip-range=100.65.0.0/24 \\
      --use-service-account-credentials=true
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kube-controller-manager*

....
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
....

Now we can do a health check of the master node
....
Kubectl get cs
....

Output
....
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}
....

*kubelet*

Its role is to run the pods on the worker node.

*Setting up Kubelet as a systemd service*

....
cat > /etc/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/bin/kubelet \\
  --allow-privileged=true \\
  --cluster-dns=100.65.0.10 \\
  --cluster-domain=cluster.local \\
  --container-runtime=docker \\
  --kubeconfig=/var/lib/kubelet/kubelet.kubeconfig \\
  --serialize-image-pulls=false \\
  --register-node=true \\
  --tls-cert-file=/srv/kubernetes/kubelet.pem \\
  --tls-private-key-file=/srv/kubernetes/kubelet-key.pem
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kubelet*

....
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
....

*Setting up Kube-proxy as a systemd service*

....
cat > /etc/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/bin/kube-proxy \\
  --cluster-cidr=100.64.0.0/16 \\
  --kubeconfig=/var/lib/kubelet/kubelet.kubeconfig \\
  --masquerade-all=true \\
  --proxy-mode=iptables
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
....

*Commands to start and check status of kube-proxy*

....
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy
....

Now we need to check whether the node has been registered or not.

....
kubectl get nodes
....

Output
....
NAME                   STATUS    ROLES     AGE       VERSION
kubeworker.novalocal   Ready     <none>    1d        v1.9.8
....

NOTE: The flags of the respective service files may get deprecated between various releases of kubernetes. If a particular component
throws an error of deprecated flag on checking the logs then please refer to the official API documentation of that particular version and modify the service file
accordingly.


Now the kubernetes cluster is ready. Let's deploy nginx app.

Run nginx
....
kubectl create deployment nginx --image=nginx
....

See pod information
....
kubectl get pods -o wide
....

Tests to be done on the worker node and master node 
....
ping <POD_IP_ADDRESS>
....

There should not be any packet loss
....
curl http://<POD_IP_ADDRESS>
....

Output
....
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
....

If you want to remove the deployment
....
kubectl delete deployment nginx
....


== Troubleshooting options
To view the latest logs of any service use journalctl -u <service_name> and press *shift + G* to view the latest entries in the log.

== References

1. https://github.com/linux-on-ibm-z/docs/wiki/Building-etcd
2. https://nixaid.com/deploying-kubernetes-cluster-from-scratch/
3. https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step/
4. https://kubernetes.io/docs/setup/scratch/
5. https://docs.docker.com/install/linux/linux-postinstall/
6. https://docs.platform9.com/support/disabling-swap-kubernetes-node/
7. https://serverfault.com/questions/881517/why-disable-swap-on-kubernetes
8. https://github.com/anujajakhade/anuja/wiki/Docker-on-RHEL
