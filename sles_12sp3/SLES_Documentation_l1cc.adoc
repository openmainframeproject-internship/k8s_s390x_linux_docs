= Installing Kubernetes on LinuxOne - SUSE Linux Enterprise Server Distribution
Rajula Vineet Reddy <rajula96reddy@gmail.com>
v2.0, 2018-08-12
:toc: left

*Note:* This setup documents running a two node (1 Master & 1 Worker) Kubernetes Cluster
on LinuxOne cloud using LinuxOne supported linux distribution *SUSE Linux Enterprise Server 12 SP3*.

Also please do note that, all the lines starting with *#* in the code blocks
are comments and are not intended to be ran, although they are harmless when ran ;).

## Versions of all the components used
The cluster is based on *docker* as container run-time and the components are installed as *systemd* services
with *flannel* as the overlay networking model. The versions of each components used are as follows
[options="header,footer",width="70%"]
|====
| Component/ Infrastructure | Version
| LinuxOne Instance flavour | LinuxONE-Medium
| LinuxOne Instance OS | SLES12SP3
//| Go | 1.10.2
| Kubernetes | 1.9.6
| Docker | 17.09.1-ce
| Etcd | 3.3.1
| Flannel | 0.9.1
|====

## Node Setup
This setup requires about 2 LinuxOne cloud instances. One for master and one for worker. Make sure you chose the SLES12SP3 flavour for both the
instances while provisioning. SSH into the nodes and follow the below steps in the nodes specified across each step

1. Set the hostnames for each of the nodes using the utility `hostnamectl` as follows. Preferrably
set the hostnames as *k8s-master* & *k8s-worker* respectively.
+
```
# On Master Node
hostnamectl set-hostname k8s-master
# On Worker Node
hostnamectl set-hostname k8s-worker
```
2. Note down the IP addresses of both the nodes. In the context of this document, IP addresses of the nodes
are as follows
+
[options="header,footer",width="50%"]
|====
| Node | IP
| Master | [red]#148.100.10.15#
| Worker | [red]#148.100.10.16#
|====
+
3. It is recommended to login as root on all the nodes, as we will be dealing with files and tools which require
root permissions. You can login as root from another account using `sudo -i`. If not, use *sudo*
before a command wherever required. Anyways, just to be safe I will include sudo where required.
4. Enable IP forward settings on all the worker node(s) so that, the containers are able to ping the internet using the following commands
+
....
sysctl -w net.ipv4.ip_forward=1
sysctl -w net.ipv4.conf.all.forwarding=1
sysctl -w net.bridge.bridge-nf-call-iptables=1
....
5. Check if docker is installed and the daemon is running on both the nodes
 by running the following commands.
+
....
sudo systemctl status docker
docker version
....
6. Create the below folders in order to save certificates & configuration files later
- On Master Node
+
....
sudo mkdir -p /srv/kubernetes
sudo mkdir -p /var/lib/{kube-controller-manager,kube-scheduler}
sudo mkdir /var/lib/flanneld/
....
+
- On Worker Node(s)
+
....
sudo mkdir -p /srv/kubernetes
sudo mkdir -p /var/lib/{kube-proxy,kubelet}
sudo mkdir /var/lib/flanneld/
....
7. It is recommended to run Kubernetes without using swap memory.
Disable swap space on the *worker* node(s), by running
+
....
sudo swapoff -a
sudo sed -i 's/.*swap/#&/' /etc/fstab
....
8. Copy the contents of SSH public key of the Master node in the `authorized_keys`
file on all the worker node(s). By doing this, we can _scp_ files into the worker
node(s) from the master node without having to authenticate.
+
....
# On the master node, check if you already have the file 'id_rsa.pub' in '.ssh/'. If not generate the key using the following steps
ssh-keygen
# Give default settings by pressing enter
cat .ssh/id_rsa.pub
#Copy the contents from the output of the above command
# On the worker node(s) paste the copied contents at the end in the file '.ssh/authorized_keys'
echo "<key_copied>" >> .ssh/authorized_keys
....
9. It is recommended to login as root on all the nodes, as we will be dealing with files and tools which require
root permissions. You can login as root from another account using `sudo -i`. If not, use *sudo*
before a command wherever required. Anyways, just to be safe I will include sudo where required.
//7. Copy the worker node ssh keys on to the master node using scp from the system you are
//logging in from. The command should be on the lines of
//+
//....
//scp -i <master_node>.pem <worker_node>.pem linux1@148.100.10.15:/home/linux1/
//....
//+
//Here *<master_node>.pem* refers to the master node ssh key and *<worker_node>.pem* refers to
//the worker node ssh key, Do change it accordingly.

## Installing the required components on the nodes
We need to download all the required components such as Kubernetes packages, Etcd, Flanneld on both the nodes.
Run the following commands on the nodes specified.

### Components on Master node
[subs=+quotes]
....
sudo zypper ar -fc https://download.opensuse.org/repositories/home:/mfriesenegger:/branches:/devel:/CaaSP:/Head:/ControllerNode/SLE_12_SP3/ kubernetes
sudo zypper refresh
# choose trust always *'a'*
sudo zypper in etcd etcdctl kubernetes-master kubernetes-client flannel
....
### Components on Worker node(s)
[subs=+quotes]
....
sudo zypper ar -fc https://download.opensuse.org/repositories/home:/mfriesenegger:/branches:/devel:/CaaSP:/Head:/ControllerNode/SLE_12_SP3/ kubernetes
sudo zypper refresh
# choose trust always *'a'*
sudo zypper in kubernetes-node flannel
....
## Creating Certificates & Authentication to enable secure communication across all the Kubernetes components
This section creates all the necessary certificates and config files to enforce secure
communication. Run all the following steps on the master node. We will have to copy
some of the certs and config files generated to the worker nodes, in order to setup the
worker components.

### Generating Certificates
#### CA - Certificate Authority
....
cd /srv/kubernetes
openssl genrsa -out ca-key.pem 2048
openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
....
#### Master Node OpenSSL config
....
cat > openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 127.0.0.1
IP.2 = 148.100.10.15 # Master IP
IP.3 = 100.65.0.1 #Service IP
EOF
....
#### Kube-apiserver certificates
....
openssl genrsa -out apiserver-key.pem 2048
openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial \
-out apiserver.pem -days 7200 -extensions v3_req -extfile openssl.cnf
....
#### Admin certificates
....
openssl genrsa -out admin-key.pem 2048
openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=admin"
openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 7200
....
#### Kube-controller-manager certificates
....
openssl genrsa -out kube-controller-manager-key.pem 2048
openssl req -new -key kube-controller-manager-key.pem -out kube-controller-manager.csr -subj "/CN=kube-controller-manager"
openssl x509 -req -in kube-controller-manager.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-controller-manager.pem -days 7200
....
#### Kube-scheduler certificates
....
openssl genrsa -out kube-scheduler-key.pem 2048
openssl req -new -key kube-scheduler-key.pem -out kube-scheduler.csr -subj "/CN=kube-scheduler"
openssl x509 -req -in kube-scheduler.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-scheduler.pem -days 7200
....
#### Worker OpenSSL config
....
cat > worker-openssl.cnf << EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = 148.100.10.16
#IP.2 If more workers are to be configured
EOF
....
#### Kubelet certificates (for Worker node)
....
openssl genrsa -out kubelet-key.pem 2048
openssl req -new -key kubelet-key.pem -out kubelet.csr -subj "/CN=kubelet" -config worker-openssl.cnf
openssl x509 -req -in kubelet.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kubelet.pem -days 7200 -extensions v3_req -extfile worker-openssl.cnf
....
#### Kube-proxy certificates (for Worker node)
....
openssl genrsa -out kube-proxy-key.pem 2048
openssl req -new -key kube-proxy-key.pem -out kube-proxy.csr -subj "/CN=kube-proxy"
openssl x509 -req -in kube-proxy.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out kube-proxy.pem -days 7200
....
#### Worker node certificates
Note: *k8sworker* here (and also in the next few steps) refers to the hostname of the worker & *148.100.10.16* refers to the worker node IP. Repeat the procedure after changing
the hostname and IP to configure more worker nodes.
....
openssl genrsa -out k8sworker-worker-key.pem 2048
WORKER_IP=148.100.10.16 openssl req -new -key k8sworker-worker-key.pem -out k8sworker-worker.csr -subj "/CN=system:node:k8sworker" -config worker-openssl.cnf
WORKER_IP=148.100.10.16 openssl x509 -req -in k8sworker-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out k8sworker-worker.pem -days 7200 -extensions v3_req -extfile worker-openssl.cnf
....
#### Etcd OpenSSL config
....
cat > etcd-openssl.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
extendedKeyUsage = clientAuth,serverAuth
subjectAltName = @alt_names
[alt_names]
IP.1 = 148.100.10.15
EOF
....
#### Etcd certificates
....
openssl genrsa -out etcd.key 2048
openssl req -new -key etcd.key -out etcd.csr -subj "/CN=etcd" -extensions v3_req -config etcd-openssl.cnf -sha256
openssl x509 -req -sha256 -CA ca.pem -CAkey ca-key.pem -CAcreateserial \
-in etcd.csr -out etcd.crt -extensions v3_req -extfile etcd-openssl.cnf -days 7200
....

#### Copy the required certificates to the Worker node
....
# Run the below step on the Master node
scp ca.pem etcd.crt etcd.key kubelet.pem kubelet-key.pem root@148.100.10.16:/srv/kubernetes/
....
// # Run the below step on the Worker node
// sudo cp /home/linux1/{*.pem,*.crt,*.key} /srv/kubernetes/
// # This is required because, the permissions associated with li1cc key doesn't allow us to directly copy to root folders
### Generating Kubeconfig files
#### Admin Kubeconfig
....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.10.15:6443
kubectl config set-credentials admin --client-certificate=/srv/kubernetes/admin.pem --client-key=/srv/kubernetes/admin-key.pem --embed-certs=true --token=$TOKEN
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=admin
kubectl config use-context linux1.k8s
cat ~/.kube/config #Create config file
....
#### Kube-controller-manager Kubeconfig
....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.10.15:6443 --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-credentials kube-controller-manager --client-certificate=/srv/kubernetes/kube-controller-manager.pem --client-key=/srv/kubernetes/kube-controller-manager-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-controller-manager --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-controller-manager/kubeconfig
....
#### Kube-scheduler Kubeconfig
....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.10.15:6443 --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-credentials kube-scheduler --client-certificate=/srv/kubernetes/kube-scheduler.pem --client-key=/srv/kubernetes/kube-scheduler-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=kube-scheduler --kubeconfig=/var/lib/kube-scheduler/kubeconfig
kubectl config use-context linux1.k8s --kubeconfig=/var/lib/kube-scheduler/kubeconfig
....
#### Kubelet Kubeconfig (for each Worker Node)
....
TOKEN=$(dd if=/dev/urandom bs=128 count=1 2>/dev/null | base64 | tr -d "=+/" | dd bs=32 count=1 2>/dev/null)
kubectl config set-cluster linux1.k8s --certificate-authority=/srv/kubernetes/ca.pem --embed-certs=true --server=https://148.100.99.99:6443 --kubeconfig=/srv/kubernetes/k8sworker-worker.kubeconfig
kubectl config set-credentials k8sworker --client-certificate=/srv/kubernetes/k8sworker-worker.pem --client-key=/srv/kubernetes/k8sworker-worker-key.pem --embed-certs=true --token=$TOKEN --kubeconfig=/srv/kubernetes/k8sworker-worker.kubeconfig
kubectl config set-context linux1.k8s --cluster=linux1.k8s --user=k8sworker --kubeconfig=/srv/kubernetes/k8sworker-worker.kubeconfig
kubectl config use-context linux1.k8s --kubeconfig=/srv/kubernetes/k8sworker-worker.kubeconfig
scp k8sworker-worker.kubeconfig root@148.100.10.16:/srv/kubernetes/kubelet.kubeconfig
....
#### Copy the required config files to the worker node(s)
Similar to how we copied the certificates, we need to copy the configurations to the worker node(s)
....
# Run the below command on the master node
scp kubelet.kubeconfig root@148.100.10.16:/var/lib/kubelet/kubeconfig
scp kube-proxy.kubeconfig root@148.100.10.16:/var/lib/kube-proxy/kubeconfig
....
// # <worker.pem> here refers to the worker node key
// # Run the below commands on the worker node
// sudo cp /home/linux1/kubelet.kubeconfig /var/lib/kubelet/kubeconfig
// sudo cp /home/linux1/kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
## Setting Up Etcd
### Modifying the Etcd systemd service & its configuration
Modify the file ``/usr/lib/systemd/system/etcd.service`` as shown below (Red indicates the modifications to the file)
[subs=+quotes]
....
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
[red]#Environment="ETCD_UNSUPPORTED_ARCH=s390x"#
EnvironmentFile=-/etc/sysconfig/etcd
User=etcd
# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/sbin/etcd --name=\"${ETCD_NAME}\"  \
--data-dir=\"${ETCD_DATA_DIR}\" \
--listen-client-urls=\"${ETCD_LISTEN_CLIENT_URLS}\" \
[red]#--cert-file=\"${ETCD_CERT_FILE}\" \
--key-file=\"${ETCD_KEY_FILE}\" \
--peer-cert-file=\"${ETCD_PEER_CERT_FILE}\" \
--peer-key-file=\"${ETCD_PEER_KEY_FILE}\" \
--trusted-ca-file=\"${ETCD_TRUSTED_CA_FILE}\"  \
--peer-trusted-ca-file=\"${ETCD_TRUSTED_CA_FILE}\"  \
--peer-client-cert-auth \
--client-cert-auth \
--initial-advertise-peer-urls=\"${ETCD_INITIAL_ADVERTISE_PEER_URLS}\"  \
--listen-peer-urls=\"${ETCD_LISTEN_PEER_URLS}\" \
--advertise-client-urls=\"${ETCD_ADVERTISE_CLIENT_URLS}\"  \
--initial-cluster-token=\"${ETCD_INITIAL_CLUSTER_TOKEN}\" \
--initial-cluster=\"${ETCD_INITIAL_CLUSTER}\" \
--initial-cluster-state=\"${ETCD_INITIAL_CLUSTER_STATE}\"#"
Restart=on-failure
LimitNOFILE=65536
Nice=-10
IOSchedulingClass=best-effort
IOSchedulingPriority=2

[Install]
WantedBy=multi-user.target
....
Also initialize the variables in the configuration file ``/etc/sysconfig/etcd``
as shown below
....
# [member]
ETCD_NAME=master
ETCD_DATA_DIR="/data/etcd"
#ETCD_WAL_DIR=""
#ETCD_SNAPSHOT_COUNT="10000"
#ETCD_HEARTBEAT_INTERVAL="100"
#ETCD_ELECTION_TIMEOUT="1000"
ETCD_LISTEN_PEER_URLS="https://148.100.10.15:2380"
# The value "http://127.0.0.1:2379" can also be used for ETCD_LISTEN_CLIENT_URLS, but it is not secure
ETCD_LISTEN_CLIENT_URLS="https://148.100.10.15:2379"
#ETCD_MAX_SNAPSHOTS="5"
#ETCD_MAX_WALS="5"
#ETCD_CORS=""
#
#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://148.100.10.15:2380"
# if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=http://..."
ETCD_INITIAL_CLUSTER="master=https://148.100.10.15:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-0"
ETCD_ADVERTISE_CLIENT_URLS="https://148.100.10.15:2379"
#ETCD_DISCOVERY=""
#ETCD_DISCOVERY_SRV=""
#ETCD_DISCOVERY_FALLBACK="proxy"
#ETCD_DISCOVERY_PROXY=""
#
#[proxy]
#ETCD_PROXY="off"
#ETCD_PROXY_FAILURE_WAIT="5000"
#ETCD_PROXY_REFRESH_INTERVAL="30000"
#ETCD_PROXY_DIAL_TIMEOUT="1000"
#ETCD_PROXY_WRITE_TIMEOUT="5000"
#ETCD_PROXY_READ_TIMEOUT="0"
#
#[security]
ETCD_CERT_FILE="/srv/kubernetes/etcd.crt"
ETCD_KEY_FILE="/srv/kubernetes/etcd.key"
ETCD_CLIENT_CERT_AUTH="true"
ETCD_TRUSTED_CA_FILE="/srv/kubernetes/ca.pem"
ETCD_PEER_CERT_FILE="/srv/kubernetes/etcd.crt"
ETCD_PEER_KEY_FILE="/srv/kubernetes/etcd.key"
ETCD_PEER_CLIENT_CERT_AUTH="true"
#ETCD_PEER_TRUSTED_CA_FILE=""
#
#[logging]
ETCD_DEBUG="true"
# examples for -log-package-levels etcdserver=WARNING,security=DEBUG
ETCD_LOG_PACKAGE_LEVELS="DEBUG"
....
The default value of *ETCD_DATA_DIR* is */var/lib/etcd*, there is high
chance of running out of space. In that we need to change the *ETCD_DATA_DIR*
variable in `/var/sysconfig/etcd` file and also the new folder need to be
given appropriate permissions for the *etcd* user (as the systemd file is running as etcd user)
using the command `chown -R etcd:etcd <new_folder_path`>.
In the above config I have assigned `/data/etcd` as the data directory. So I need to create
the directory and give owner permissions to *etcd* user
....
mkdir -p /data/etcd
chown -R etcd:etcd /data/etcd
....
Also, you need to modify the *working directory* value to the new directory in the etcd service file which would be
`/usr/lib/systemd/system/etcd.service`.

Now, we need to flush the IP tables in order for Etcd to run without issues. It can be done by running the following command
....
iptables -F
....
// or iptables REJECT FORWARD drop
However, when we setup flannel in the next step, it will add more iptable rules required.

Now, run the following commands to start *etcd*
....
sudo systemctl daemon-reload
sudo systemctl enable etcd
sudo systemctl start etcd
....

### Test etcd cluster
```
etcdctl --endpoints https://148.100.10.15:2379 --cert-file /srv/kubernetes/etcd.crt --key-file /srv/kubernetes/etcd.key --ca-file /srv/kubernetes/ca.pem cluster-health
```
This should return *cluster is healthy* if etcd is running correctly.

## Setting up Flannel
Flannel should be installed on all the nodes. Do the following steps
on all the nodes.

### Adding an entry to etcd
This should be run only once and only on the Master node
....
etcdctl --endpoints https://148.100.10.15:2379 --cert-file /srv/kubernetes/etcd.crt --key-file /srv/kubernetes/etcd.key --ca-file /srv/kubernetes/ca.pem set /coreos.com/network/config '{ "Network": "100.64.0.0/16", "SubnetLen": 24, "Backend": {"Type": "vxlan"} }'
....
### Configure Flannel Settings
Initialize the variables required for flanneld in the configuration
file ``/etc/sysconfig/flanneld`` as shown below
....
# Flanneld configuration options

# etcd url location.  Point this to the server where etcd runs
FLANNEL_ETCD_ENDPOINTS="https://148.100.10.15:2379"
# ETCD Prefix for the -etcd-prefix argument
FLANNEL_ETCD_KEY="/coreos.com/network"
# Any additional options that you want to pass
FLANNEL_OPTIONS="-iface=eth0 -subnet-file=/var/lib/flanneld/subnet.env \
-etcd-cafile=/srv/kubernetes/ca.pem \
-etcd-certfile=/srv/kubernetes/etcd.crt \
-etcd-keyfile=/srv/kubernetes/etcd.key \
-ip-masq=true"
....
The 'iface' here should be the interface through which the nodes communicate and flannel will
be configured for this interface.

Start the flannel daemon using the following commands
....
sudo systemctl enable flanneld
sudo systemctl start flanneld
....

#### Configure Docker Settings
Modify the docker configuration file ``/etc/sysconfig/docker`` to
add extra arguments for docker executable as follows
....
## Path           : System/Management
## Description    : Extra cli switches for docker daemon
## Type           : string
## Default        : ""
## ServiceRestart : docker
#
DOCKER_OPTS="--bip=100.64.98.1/24 --mtu=1450 --iptables=false --ip-masq=false --ip-forward=true"
....
For the values of _bip_,_mtu_ refer to the file `/var/lib/flannel/subnet.env` on each of the nodes separately.
_bip_ is the value of the variable _FLANNEL_SUBNET_ and _mtu_ is the value of
the variable _FLANNEL_MTU_ in the above file.

Then run the following commands
....
sudo systemctl daemon-reload
sudo systemctl restart docker
....
### Testing flanneld
Once *flanneld* is started and *docker* daemon is restarted, running ```route -n``` on Master node
and Worker node(s) the bridge established can be seen with the interface name as 'flannelx'. Also
the IP of the nodes on the flannel networks can be seen by running ```ip a``` on all the nodes. If in case
you are not able to ping any IP's on the flannel network, try running `iptables -F` and check. By flushing
the ip tables, we will be clearing any unwanted filtering of the packets.

## Setting up Kubernetes components on the Nodes
### Master Components
Modify the following configuration files in the directory ``/etc/kubernetes/`` as shown below

#### General system config -> ``/etc/kubernetes/config``
....
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=5"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=true"

# How the controller-manager, and proxy find the apiserver
KUBE_MASTER="--master=https://148.100.10.15:6443"
....
#### Api-server config ->  ``/etc/kubernetes/apiserver``
....
###
# kubernetes system config
#
# The following values are used to configure the kube-apiserver
#

# The address on the local server to listen to.
KUBE_API_ADDRESS=""

# The port on the local server to listen on.
# KUBE_API_PORT="--port=8080"

# Port minions listen on
# KUBELET_PORT="--kubelet-port=10250"

# Comma separated list of nodes in the etcd cluster
# KUBE_ETCD_SERVERS="--etcd-servers=https://148.100.10.15:2379"

# Address range to use for services
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=100.65.0.0/24"

# default admission control policies
KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota"

# Add your own!
KUBE_API_ARGS="--bind-address=0.0.0.0 \
--advertise-address=148.100.10.15 \
--anonymous-auth=false \
--apiserver-count=1 \
--authorization-mode=Node,AlwaysAllow,RBAC \
--authorization-rbac-super-user=admin \
--etcd-cafile=/srv/kubernetes/ca.pem \
--etcd-certfile=/srv/kubernetes/etcd.crt \
--etcd-keyfile=/srv/kubernetes/etcd.key \
--etcd-servers=https://148.100.10.15:2379 \
--enable-swagger-ui=false \
--event-ttl=1h \
--kubelet-certificate-authority=/srv/kubernetes/ca.pem \
--kubelet-client-certificate=/srv/kubernetes/kubelet.pem \
--kubelet-client-key=/srv/kubernetes/kubelet-key.pem \
--kubelet-https=true \
--client-ca-file=/srv/kubernetes/ca.pem \
--runtime-config=api/all=true,batch/v2alpha1=true,rbac.authorization.k8s.io/v1alpha1=true \
--service-node-port-range=30000-32767 \
--secure-port=6443 \
--storage-backend=etcd3 \
--tls-cert-file=/srv/kubernetes/apiserver.pem \
--tls-private-key-file=/srv/kubernetes/apiserver-key.pem \
--tls-ca-file=/srv/kubernetes/ca.pem"
....
#### Scheduler config ->  ``/etc/kubernetes/scheduler``
....
###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS="--leader-elect=true \
--kubeconfig=/var/lib/kube-scheduler/kubeconfig"
....
#### Controller_manager config ->  ``/etc/kubernetes/controller-manager``
....
###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS="--allocate-node-cidrs=true \
--address=0.0.0.0 \
--attach-detach-reconcile-sync-period=1m0s \
--cluster-cidr=100.64.0.0/16 \
--cluster-name=k8s.virtual.local \
--leader-elect=true \
--root-ca-file=/srv/kubernetes/ca.pem \
--service-account-private-key-file=/srv/kubernetes/apiserver-key.pem \
--use-service-account-credentials=true \
--kubeconfig=/var/lib/kube-controller-manager/kubeconfig \
--cluster-signing-cert-file=/srv/kubernetes/ca.pem \
--cluster-signing-key-file=/srv/kubernetes/ca-key.pem \
--service-cluster-ip-range=100.65.0.0/24 \
--configure-cloud-routes=false"
....
#### Start the master components
....
sudo systemctl enable kube-apiserver
sudo systemctl start kube-apiserver
sudo systemctl enable kube-controller-manager
sudo systemctl start kube-controller-manager
sudo systemctl enable kube-scheduler
sudo systemctl start kube-scheduler
....
### Worker Components
#### General system config ->  ``/etc/kubernetes/config``
....
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=2"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=true"

# How the controller-manager, and proxy find the apiserver
KUBE_MASTER="--master=https://148.100.10.15:6443"
....
#### Kubelet config ->  ``/etc/kubernetes/kubelet``
....
###
# kubernetes kubelet (minion) config

# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=148.100.10.16"

# The port for the info server to serve on
# KUBELET_PORT="--port=10250"

# You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=148.100.10.16"

# Add your own!
KUBELET_ARGS="--pod-manifest-path=/etc/kubernetes/manifests \
--kubeconfig=/srv/kubernetes/kubelet.kubeconfig \
--tls-cert-file=/srv/kubernetes/kubelet.pem \
--tls-private-key-file=/srv/kubernetes/kubelet-key.pem \
--cert-dir=/var/lib/kubelet \
--container-runtime=docker \
--serialize-image-pulls=false \
--register-node=true \
--cluster-dns=100.65.0.10 \
--cluster-domain=cluster.local \
--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice \
--eviction-hard=memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<5% \
--docker=unix:///var/run/docker.sock \
--node-labels=kubernetes.io/role=master,node-role.kubernetes.io/master="
....
#### Kube_proxy config ->  ``/etc/kubernetes/proxy``
....
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS="--cluster-cidr=100.64.0.0/16 \
--masquerade-all=true \
--kubeconfig=/srv/kubernetes/kubelet.kubeconfig \
--proxy-mode=iptables"
....
#### Start the worker components
....
sudo systemctl enable kubelet
sudo systemctl start kubelet
sudo systemctl enable kube-proxy
sudo systemctl start kube-proxy
....
## Testing the cluster
Now that we have deployed the cluster let's test it.

### Test if Kuberenetes Api Server is Running
Running ```kubectl version``` should return the version of both kubectl and kube-api-server
....
Client Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-04-06T19:54:24Z", GoVersion:"go1.9.4", Compiler:"gc", Platform:"linux/s390x"}
Server Version: version.Info{Major:"1", Minor:"9", GitVersion:"v1.9.6", GitCommit:"9f8ebd171479bec0ada837d7ee641dec2f8c6dd1", GitTreeState:"clean", BuildDate:"2018-04-06T19:54:24Z", GoVersion:"go1.9.4", Compiler:"gc", Platform:"linux/s390x"}
....
### Test if all the components are healthy
Running ```kubectl get componentstatus``` should return the status of all the components
....
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health":"true"}
....
### Test if the node is registered
Running ```kubectl get nodes``` should return the nodes sucessfully registered with the server and status of each node.
....
NAME         STATUS    ROLES     AGE       VERSION
148.100.10.16   Ready     <none>    6d        v1.9.6
....
### Deploy ngninx
Let's run an Ngnix app on the cluster.
....
kubectl run nginx --image=nginx --port=80
kubectl get pods -o wide
curl http://100.64.80.1 #The IP of any of the nginx pod from the above command
#You should get the nginx welcome page as the result if everything is working fine
....
## Setting up Kube-dns
I will use the following yaml derived from the official kubernetes repository,
made some changes (which are highlighted in red). Make sure the clusterIP here
is same as what we provide as parameters to the Kubernetes components. The yaml file can be directly download
from https://github.com/rajula96reddy/LinuxOne_Kubernetes_SLES_Deployment_Documentation/blob/master/kube-dns.yaml[here].
[subs="quotes"]
....
# Copyright 2016 The Kubernetes Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "KubeDNS"
spec:
  selector:
    k8s-app: kube-dns
  [red]#clusterIP: 100.65.0.10#
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 0
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      tolerations:
      - key: "CriticalAddonsOnly"
        operator: "Exists"
      volumes:
      - name: kube-dns-config
        configMap:
          name: kube-dns
          optional: true
      [red]#- hostPath:
          path: /srv/kubernetes
        name: ssl-certs-kubernetes#
      containers:
      - name: kubedns
        image: gcr.io/google_containers/k8s-dns-kube-dns-s390x:1.14.7
        resources:
          # TODO: Set memory limits when we've profiled the container for large
          # clusters, then set request = limit to keep this container in
          # guaranteed class. Currently, this container falls into the
          # "burstable" category so the kubelet doesn't backoff from restarting it.
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        livenessProbe:
          httpGet:
            path: /healthcheck/kubedns
            port: 10054
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /readiness
            port: 8081
            scheme: HTTP
          # we poll on pod startup for the Kubernetes master service and
          # only setup the /readiness HTTP server once that's available.
          initialDelaySeconds: 3
          timeoutSeconds: 5
        args:
        - --domain=cluster.local.
        - --dns-port=10053
        [red]#- --kube-master-url=https://148.100.10.15:6443
        - --config-dir=/kube-dns-config
        - --kubecfg-file=/srv/kubernetes/kubelet.kubeconfig#
        - --v=2
        env:
        - name: PROMETHEUS_PORT
          value: "10055"
        ports:
        - containerPort: 10053
          name: dns-local
          protocol: UDP
        - containerPort: 10053
          name: dns-tcp-local
          protocol: TCP
        - containerPort: 10055
          name: metrics
          protocol: TCP
        volumeMounts:
        - name: kube-dns-config
          mountPath: /kube-dns-config
        [red]#- name: ssl-certs-kubernetes
          mountPath: /srv/kubernetes
          readOnly: true#
      - name: dnsmasq
        image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-s390x:1.14.7
        livenessProbe:
          httpGet:
            path: /healthcheck/dnsmasq
            port: 10054
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        args:
        - -v=2
        - -logtostderr
        - -configDir=/etc/k8s/dns/dnsmasq-nanny
        - -restartDnsmasq=true
        - --
        - -k
        - --cache-size=1000
        - --no-negcache
        - --log-facility=-
        - --server=/cluster.local/127.0.0.1#10053
        - --server=/in-addr.arpa/127.0.0.1#10053
        - --server=/ip6.arpa/127.0.0.1#10053
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
        resources:
          requests:
            cpu: 150m
            memory: 20Mi
        volumeMounts:
        - name: kube-dns-config
          mountPath: /etc/k8s/dns/dnsmasq-nanny
      - name: sidecar
        image: gcr.io/google_containers/k8s-dns-sidecar-s390x:1.14.7
        livenessProbe:
          httpGet:
            path: /metrics
            port: 10054
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        args:
        - --v=2
        - --logtostderr
        - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
        - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
        ports:
        - containerPort: 10054
          name: metrics
          protocol: TCP
        resources:
          requests:
            memory: 20Mi
            cpu: 10m
      dnsPolicy: Default  # Don't use cluster DNS.
      serviceAccountName: kube-dns
....
To deploy the kube-dns pod, save the yaml as say `kube-dns.yaml` and
run the command `kubectl apply -f kube-dns.yaml`. In a few minutes,
the pods will start running. Run the following commands to verify if
kubedns is running as intended.
....
kubectl create -f https://k8s.io/examples/admin/dns/busybox.yaml
#Wait for the pod to start
kubectl exec -ti busybox -- nslookup kubernetes.default.svc.cluster.local
# This should result in returning the Kubernetes cluster IP
....
If in case the 'nslookup' is throwing up an error saying can't reach server, try stopping the `systemd-resolve` service and try.

## Troubleshooting
- If any of the Kubernetes component throws up an error, check the reason for the error by observing the logs
of the service using ```journalctl -fu <service name>```
- To debug a kubectl command, use the flag ```-v=<log level>```
- Get logs of a pod using 'kubectl logs'
- Exec into a pod in case you want to dabble with the service or app directly
- Specific container 'exec' or 'logs' can be looked at by directly using the 'docker' commands and the appropriate container name or ID

## References
- https://github.com/linux-on-ibm-z/docs/wiki/Building-etcd
- https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step/
- https://github.com/kelseyhightower/kubernetes-the-hard-way/tree/2983b28f13b294c6422a5600bb6f14142f5e7a26/docs
- https://nixaid.com/deploying-kubernetes-cluster-from-scratch/
- https://kubernetes.io
- https://www.suse.com/documentation/sles-12/singlehtml/book_sles_docker/book_sles_docker.html
